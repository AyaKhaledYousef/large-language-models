{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQlQ4ZWgcXDW"
      },
      "outputs": [],
      "source": [
        "!pip install -Uqqq pip --progress-bar off\n",
        "!pip install -qqq torch==2.0.1 --progress-bar off\n",
        "!pip install -qqq transformers==4.31.0 --progress-bar off\n",
        "!pip install -qqq langchain==0.0.266 --progress-bar off\n",
        "!pip install -qqq chromadb==0.4.5 --progress-bar off\n",
        "!pip install -qqq pypdf==3.15.0 --progress-bar off\n",
        "!pip install -qqq xformers==0.0.20 --progress-bar off\n",
        "!pip install -qqq sentence_transformers==2.2.2 --progress-bar off\n",
        "!pip install -qqq InstructorEmbedding==1.0.1 --progress-bar off\n",
        "!pip install -qqq pdf2image==1.16.3 --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/PanQiWei/AutoGPTQ/releases/download/v0.4.1/auto_gptq-0.4.1+cu118-cp310-cp310-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "ecTRR8hwhn0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq auto_gptq-0.4.1+cu118-cp310-cp310-linux_x86_64.whl --progress-bar off"
      ],
      "metadata": {
        "id": "C0_Fa50LkfoB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e73a57be-5c5c-4c10-a639-cb0793709908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install poppler-utils"
      ],
      "metadata": {
        "id": "t7ShwfeTubDU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2b96b11-f400-49f6-8313-f196b0e1d57b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "from langchain import HuggingFacePipeline, PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from pdf2image import convert_from_path\n",
        "from transformers import AutoTokenizer, TextStreamer, pipeline\n",
        "\n",
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "vH5IpjeNflMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks.base import BaseCallbackHandler\n",
        "from langchain.schema import LLMResult\n",
        "from typing import Any, Union,Dict, List\n",
        "from queue import SimpleQueue\n",
        "\n",
        "q = SimpleQueue()\n",
        "\n",
        "job_done = object() # signals the processing is done\n",
        "\n",
        "class StreamingGradioCallbackHandler(BaseCallbackHandler):\n",
        "    def __init__(self, q: SimpleQueue):\n",
        "        self.q = q\n",
        "\n",
        "    def on_llm_start(\n",
        "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
        "    ) -> None:\n",
        "        \"\"\"Run when LLM starts running. Clean the queue.\"\"\"\n",
        "        while not self.q.empty():\n",
        "            try:\n",
        "                self.q.get(block=False)\n",
        "            except q.empty():\n",
        "                continue\n",
        "\n",
        "    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n",
        "        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\n",
        "        self.q.put(token)\n",
        "\n",
        "    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
        "        \"\"\"Run when LLM ends running.\"\"\"\n",
        "        self.q.put(job_done)\n",
        "\n",
        "    def on_llm_error(\n",
        "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
        "    ) -> None:\n",
        "        \"\"\"Run when LLM errors.\"\"\"\n",
        "        self.q.put(job_done)"
      ],
      "metadata": {
        "id": "Ylnt53Wa02wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "jiX57tzWOJAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meta_images = convert_from_path(\"pdfs/meta-earnings-report.pdf\", dpi=88)\n",
        "nvidia_images = convert_from_path(\"pdfs/nvidia-earnings-report.pdf\", dpi=88)\n",
        "tesla_images = convert_from_path(\"pdfs/tesla-earnings-report.pdf\", dpi=88)\n",
        "loader = PyPDFDirectoryLoader(\"pdfs\")\n",
        "docs = loader.load()\n",
        "len(docs)"
      ],
      "metadata": {
        "id": "a9X4Fbv8ugOW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4319b764-f9b4-4cbb-ae7c-53e019b42807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceInstructEmbeddings(\n",
        "    model_name=\"hkunlp/instructor-large\", model_kwargs={\"device\": DEVICE}\n",
        ")"
      ],
      "metadata": {
        "id": "6ywUvMDqZFf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5b90f9d-1745-426c-cc5a-951e752314da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
        "texts = text_splitter.split_documents(docs)\n",
        "len(texts)"
      ],
      "metadata": {
        "id": "yFztsob3WO3x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec291009-78b4-4823-a000-cb1674abc625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "355"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "db = Chroma.from_documents(texts, embeddings, persist_directory=\"db\")"
      ],
      "metadata": {
        "id": "ESFJOOYEWTX9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cca38596-7cb4-4577-8721-b1902769a127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 20.5 s, sys: 404 ms, total: 20.9 s\n",
            "Wall time: 24.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Llama 2 13B"
      ],
      "metadata": {
        "id": "GcylvlF60r_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
        "model_basename = \"model\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "\n",
        "model = AutoGPTQForCausalLM.from_quantized(\n",
        "    model_name_or_path,\n",
        "    revision=\"gptq-4bit-128g-actorder_True\",\n",
        "    model_basename=model_basename,\n",
        "    use_safetensors=True,\n",
        "    trust_remote_code=True,\n",
        "    inject_fused_attention=False,\n",
        "    device=DEVICE,\n",
        "    quantize_config=None,\n",
        ")"
      ],
      "metadata": {
        "id": "DrSTLMQbfjFp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "392c073c-544d-4027-9154-13bc8678ddd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:auto_gptq.modeling._base:Exllama kernel is not installed, reset disable_exllama to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
            "WARNING:auto_gptq.modeling._base:CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
            "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
            "2. You are using pytorch without CUDA support.\n",
            "3. CUDA and nvcc are not installed in your device.\n",
            "WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda:CUDA extension not installed.\n",
            "WARNING:auto_gptq.nn_modules.fused_llama_mlp:skip module injection for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
        "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
        "\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "def generate_prompt(prompt: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
        "    return f\"\"\"\n",
        "[INST] <<SYS>>\n",
        "{system_prompt}\n",
        "<</SYS>>\n",
        "\n",
        "{prompt} [/INST]\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "wnXqtUHgzHOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from queue import Queue\n",
        "\n",
        "class StreamingGradioCallbackHandler(BaseCallbackHandler):\n",
        "    def __init__(self, q: Queue,job_done : object):\n",
        "        self.q = q\n",
        "        self.job_done=job_done\n",
        "\n",
        "    def on_llm_start(\n",
        "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
        "    ) -> None:\n",
        "        \"\"\"Run when LLM starts running. Clean the queue.\"\"\"\n",
        "        while not self.q.empty():\n",
        "            try:\n",
        "                self.q.get(block=False)\n",
        "            except Queue.empty():\n",
        "                continue\n",
        "\n",
        "    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n",
        "        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\n",
        "        self.q.put(token)\n",
        "\n",
        "    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
        "        \"\"\"Run when LLM ends running.\"\"\"\n",
        "        self.q.put(self.job_done)\n",
        "\n",
        "\n",
        "    def on_llm_error(\n",
        "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
        "    ) -> None:\n",
        "        \"\"\"Run when LLM errors.\"\"\"\n",
        "        self.q.put(self.job_done)"
      ],
      "metadata": {
        "id": "9E4qVPuDP7zO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "Mqw-6JyMQisZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, TextStreamer, pipeline\n",
        "\n",
        "def generate_response(user_input,history):\n",
        "  # print(\"@@@@@@@@@@@\")\n",
        "\n",
        "  global COUNT,url,last_url,chain_arabic,chain_english\n",
        "  streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "  text_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=1024,\n",
        "    temperature=0,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.15,\n",
        "    streamer=streamer,\n",
        ")\n",
        "\n",
        "\n",
        "  SYSTEM_PROMPT = \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\n",
        "  print(SYSTEM_PROMPT)\n",
        "  template = generate_prompt(\n",
        "      \"\"\"\n",
        "  {context}\n",
        "\n",
        "  Question: {question}\n",
        "  \"\"\",\n",
        "      system_prompt=SYSTEM_PROMPT,\n",
        "  )\n",
        "  # print(\"template = \", template)\n",
        "  llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"temperature\": 0})\n",
        "  prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "  qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever(search_kwargs={\"k\": 2}),\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")\n",
        "  print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
        "  # result = qa_chain(\"What is the per share revenue for Nvidia during 2023?\")\n",
        "  # print(r\"[+] result = {result}\")\n",
        "  # result =\n",
        "  return qa_chain(user_input)\n"
      ],
      "metadata": {
        "id": "g9trUT_2y_q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import gc\n",
        "# torch.cuda.empty_cache()\n",
        "# gc.collect()"
      ],
      "metadata": {
        "id": "iSFQRWbdIiSv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30843ed4-cda8-4da4-f7f4-66ffa95c4a13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio==3.48.0"
      ],
      "metadata": {
        "id": "NL1gezLpYRfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "with gr.Blocks(fill_height=True) as demo:\n",
        "\n",
        "    chatbot = gr.ChatInterface(fn=generate_response)\n",
        "\n",
        "demo.queue().launch(share=True,debug = True)"
      ],
      "metadata": {
        "id": "XtsGGoYUMGop",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e667d96-4e64-47c3-f7f8-dbf05484f48e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-7ef064737ed0>:2: GradioUnusedKwargWarning: You have unused kwarg parameters in Blocks, please remove them: {'fill_height': True}\n",
            "  with gr.Blocks(fill_height=True) as demo:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://5f219ce76c406d58e5.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5f219ce76c406d58e5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://5f219ce76c406d58e5.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_response(\"What is the per share revenue for Nvidia during 2023?\")"
      ],
      "metadata": {
        "id": "LYJNZ_93Gm3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat with Multiple PDFs"
      ],
      "metadata": {
        "id": "J7IWZ6H1ax4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from threading import Thread\n",
        "\n",
        "from gradio.themes.utils.colors import Color\n",
        "\n",
        "\n",
        "DESCRIPTION = \"\"\"\n",
        "# Llama2 13B Chat 🗨️\n",
        "This is a streaming Chat Interface implementation of [Llama2](https://huggingface.co/meta-llama)\n",
        "\"\"\"\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox()\n",
        "    clear = gr.ClearButton([msg, chatbot])\n",
        "\n",
        "# gr.Interface(fn=generate_response,\n",
        "#              inputs = [\"text\"],\n",
        "#              outputs = [\"text\"]).launch(debug = True , share =True)\n",
        "\n",
        "\n",
        "msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n",
        "\n"
      ],
      "metadata": {
        "id": "_BR5nBf_fPao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3ecacbd-3467-423e-82a6-f9b59daccb5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'respond' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-285c4e6898a4>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrespond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchatbot\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchatbot\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'respond' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5NjuCIEQIdMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(result[\"source_documents\"])"
      ],
      "metadata": {
        "id": "UyyiaN0yZ6LN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result[\"source_documents\"][0].page_content)"
      ],
      "metadata": {
        "id": "60qpCy_haA3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[\"source_documents\"][0]"
      ],
      "metadata": {
        "id": "rd9bXa1KrWG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = qa_chain(\"What is the per share revenue for Tesla during 2023?\")"
      ],
      "metadata": {
        "id": "nKmLM4UTfVmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = generate_response(\"What is the per share revenue for Nvidia during 2023?\")"
      ],
      "metadata": {
        "id": "1k96dxrgfjMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result[\"source_documents\"][1].page_content)"
      ],
      "metadata": {
        "id": "kpMIspiSahlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JkCRKUQaKXu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = qa_chain(\"What is the estimated YOY revenue for Meta during 2023?\")"
      ],
      "metadata": {
        "id": "waIck-yefydp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = qa_chain(\"What is the estimated YOY revenue for Tesla during 2023?\")"
      ],
      "metadata": {
        "id": "nq6cf8vof6wM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = qa_chain(\"What is the estimated YOY revenue for Nvidia during 2023?\")"
      ],
      "metadata": {
        "id": "SIFTDoxzgY07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = qa_chain(\n",
        "    \"Which company is more profitable during 2023 Meta, Nvidia or Tesla and why?\"\n",
        ")"
      ],
      "metadata": {
        "id": "cxp167huggFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = qa_chain(\n",
        "    \"Choose one company to invest (Tesla, Nvidia or Meta) to maximize your profits for the long term (10+ years)?\"\n",
        ")"
      ],
      "metadata": {
        "id": "vRuJYwSbgpij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "- [Tesla Quarterly Report (Jul 21, 2023)](https://ir.tesla.com/_flysystem/s3/sec/000095017023033872/tsla-20230630-gen.pdf)\n",
        "- [Meta Q2 2023 Earnings (Jul 26, 2023)](https://s21.q4cdn.com/399680738/files/doc_financials/2023/q2/Meta-06-30-2023-Exhibit-99-1-FINAL.pdf)\n",
        "- [Nvidia Fiscal Q1 2024](https://s201.q4cdn.com/141608511/files/doc_financials/2024/q1/ecefb2b2-efcb-45f3-b72b-212d90fcd873.pdf)"
      ],
      "metadata": {
        "id": "hdyKOEdRg2hc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarization :"
      ],
      "metadata": {
        "id": "2-DHl4F7wEcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary = pipeline(\n",
        "    \"summarization\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=10,\n",
        "    temperature=0,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.15,\n",
        "    streamer=streamer,\n",
        ")\n",
        "\n",
        "llm_summary = HuggingFacePipeline(pipeline=summary, model_kwargs={\"temperature\": 0})\n",
        "qa_chain_summary = RetrievalQA.from_chain_type(\n",
        "    llm=llm_summary,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever(search_kwargs={\"k\": 2}),\n",
        "    return_source_documents=True\n",
        "    # chain_type_kwargs={\"prompt\": prompt},\n",
        ")\n",
        "result_summary = qa_chain_summary(\"What is the per share revenue for Meta during 2023?\")\n"
      ],
      "metadata": {
        "id": "hC2zYykXt_ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "-lBoH-UHL_SF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "unbZzJB7VVuD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}