{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YM7p9JR_X7K3"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install fastapi\n",
        "!pip install \"langserve[all]\"\n",
        "# !pip install pydantic==1.10.13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4gHZQs-Ft9_o"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYDxZSedYWbY",
        "outputId": "683ed268-b706-422b-e702-0ed98f90010c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "HUGGINGFACEHUB_API_TOKEN = getpass()\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]= HUGGINGFACEHUB_API_TOKEN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frVimi0iX8dZ",
        "outputId": "6798b9f4-5198-4533-9d7f-ed474cc337bb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [690]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " __          ___      .__   __.   _______      _______. _______ .______     ____    ____  _______\n",
            "|  |        /   \\     |  \\ |  |  /  _____|    /       ||   ____||   _  \\    \\   \\  /   / |   ____|\n",
            "|  |       /  ^  \\    |   \\|  | |  |  __     |   (----`|  |__   |  |_)  |    \\   \\/   /  |  |__\n",
            "|  |      /  /_\\  \\   |  . `  | |  | |_ |     \\   \\    |   __|  |      /      \\      /   |   __|\n",
            "|  `----./  _____  \\  |  |\\   | |  |__| | .----)   |   |  |____ |  |\\  \\----.  \\    /    |  |____\n",
            "|_______/__/     \\__\\ |__| \\__|  \\______| |_______/    |_______|| _| `._____|   \\__/     |_______|\n",
            "\n",
            "\u001b[1;32;40mLANGSERVE:\u001b[0m Playground for chain \"/Test/\" is live at:\n",
            "\u001b[1;32;40mLANGSERVE:\u001b[0m  │\n",
            "\u001b[1;32;40mLANGSERVE:\u001b[0m  └──> /Test/playground/\n",
            "\u001b[1;32;40mLANGSERVE:\u001b[0m\n",
            "\u001b[1;32;40mLANGSERVE:\u001b[0m See all available routes at /docs/\n",
            "\n",
            "\u001b[1;31;40mLANGSERVE:\u001b[0m ⚠️ Using pydantic 2.6.4. OpenAPI docs for invoke, batch, stream, stream_log endpoints will not be generated. API endpoints and playground should work as expected. If you need to see the docs, you can downgrade to pydantic 1. For example, `pip install pydantic==1.10.13`. See https://github.com/tiangolo/fastapi/issues/10360 for details.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain.pydantic_v1 import BaseModel , Field\n",
        "#!/usr/bin/env python\n",
        "from fastapi import FastAPI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatAnthropic, ChatOpenAI\n",
        "from langserve import add_routes\n",
        "from langchain import PromptTemplate , HuggingFaceHub , LLMChain\n",
        "\n",
        "app = FastAPI(\n",
        "    title=\"LangChain Server\",\n",
        "    version=\"1.0\",\n",
        "    description=\"A simple api server using Langchain's Runnable interfaces\",\n",
        ")\n",
        "\n",
        "\n",
        "template = \"\"\" Question :{questio}\n",
        "  Answer: Let's think step by step\n",
        "  \"\"\"\n",
        "\n",
        "model = HuggingFaceHub(repo_id= 'mistralai/Mixtral-8x7B-Instruct-v0.1')\n",
        "prompt = PromptTemplate (template=template , input_variables= {'Qusetion'})\n",
        "\n",
        "add_routes(\n",
        "    app,\n",
        "    prompt | model,\n",
        "    path=\"/Test\",\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "\n",
        "    uvicorn.run(app, host=\"127.0.0.1\", port=8000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWMJYHHy0mQK"
      },
      "outputs": [],
      "source": [
        "# client.py\n",
        "import requests\n",
        "\n",
        "response = requests.post(\"http://localhost:8000/Test/invoke\",\n",
        "                         json={'input': {'topic':\"my best friend\"}})\n",
        "\n",
        "\n",
        "print(response.json())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8VbqIifum9i"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
