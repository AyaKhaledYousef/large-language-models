{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline, logging\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import argparse\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting auto-gptq==0.2.2\n",
      "  Using cached auto_gptq-0.2.2.tar.gz (52 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: accelerate>=0.19.0 in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from auto-gptq==0.2.2) (0.22.0)\n",
      "Requirement already satisfied: datasets in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from auto-gptq==0.2.2) (2.18.0)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from auto-gptq==0.2.2) (1.24.3)\n",
      "Requirement already satisfied: rouge in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from auto-gptq==0.2.2) (1.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from auto-gptq==0.2.2) (2.2.1)\n",
      "Requirement already satisfied: safetensors in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from auto-gptq==0.2.2) (0.4.2)\n",
      "Requirement already satisfied: transformers>=4.26.1 in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from auto-gptq==0.2.2) (4.39.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from accelerate>=0.19.0->auto-gptq==0.2.2) (23.2)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/.local/lib/python3.10/site-packages (from accelerate>=0.19.0->auto-gptq==0.2.2) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from accelerate>=0.19.0->auto-gptq==0.2.2) (6.0.1)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.2.2) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.2.2) (4.10.0)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.2.2) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.2.2) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.2.2) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.2.2) (2024.2.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from transformers>=4.26.1->auto-gptq==0.2.2) (0.20.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from transformers>=4.26.1->auto-gptq==0.2.2) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from transformers>=4.26.1->auto-gptq==0.2.2) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from transformers>=4.26.1->auto-gptq==0.2.2) (0.15.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from transformers>=4.26.1->auto-gptq==0.2.2) (4.66.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from datasets->auto-gptq==0.2.2) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from datasets->auto-gptq==0.2.2) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from datasets->auto-gptq==0.2.2) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from datasets->auto-gptq==0.2.2) (2.2.1)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from datasets->auto-gptq==0.2.2) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from datasets->auto-gptq==0.2.2) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from datasets->auto-gptq==0.2.2) (3.9.3)\n",
      "Requirement already satisfied: six in /home/ubuntu/.local/lib/python3.10/site-packages (from rouge->auto-gptq==0.2.2) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq==0.2.2) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq==0.2.2) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq==0.2.2) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq==0.2.2) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq==0.2.2) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq==0.2.2) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from requests->transformers>=4.26.1->auto-gptq==0.2.2) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from requests->transformers>=4.26.1->auto-gptq==0.2.2) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from requests->transformers>=4.26.1->auto-gptq==0.2.2) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from requests->transformers>=4.26.1->auto-gptq==0.2.2) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->auto-gptq==0.2.2) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas->datasets->auto-gptq==0.2.2) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from pandas->datasets->auto-gptq==0.2.2) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from pandas->datasets->auto-gptq==0.2.2) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from sympy->torch>=1.13.0->auto-gptq==0.2.2) (1.3.0)\n",
      "Building wheels for collected packages: auto-gptq\n",
      "  Building wheel for auto-gptq (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[105 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m appending conda cuda include dir /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/nvidia/cuda_runtime/include\n",
      "  \u001b[31m   \u001b[0m include dirs are: ['autogptq_cuda', '/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/nvidia/cuda_runtime/include']\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m /home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/torch/utils/cpp_extension.py:500: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(msg.format('we could not find ninja.'))\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/auto_gptq\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/__init__.py -> build/lib.linux-x86_64-cpython-310/auto_gptq\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/auto_gptq/nn_modules\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/nn_modules/fused_gptj_attn.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/nn_modules\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/nn_modules/qlinear_triton.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/nn_modules\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/nn_modules/_fused_base.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/nn_modules\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/nn_modules/fused_llama_mlp.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/nn_modules\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/nn_modules/fused_llama_attn.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/nn_modules\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/nn_modules/qlinear_old.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/nn_modules\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/nn_modules/qlinear.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/nn_modules\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/nn_modules/__init__.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/nn_modules\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/auto_gptq/utils\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/utils/data_utils.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/utils\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/utils/import_utils.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/utils\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/utils/__init__.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/utils\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/auto_gptq/eval_tasks\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/eval_tasks/sequence_classification_task.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/eval_tasks\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/eval_tasks/_base.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/eval_tasks\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/eval_tasks/language_modeling_task.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/eval_tasks\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/eval_tasks/text_summarization_task.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/eval_tasks\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/eval_tasks/__init__.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/eval_tasks\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/auto_gptq/quantization\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/quantization/quantizer.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/quantization\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/quantization/gptq.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/quantization\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/quantization/__init__.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/quantization\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/auto_gptq/modeling\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/modeling/gpt_neox.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/modeling\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/modeling/_const.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/modeling\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/modeling/_utils.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/modeling\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/modeling/codegen.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/modeling\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/modeling/auto.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/modeling\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/modeling/opt.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/modeling\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/modeling/bloom.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/modeling\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/modeling/_base.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/modeling\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/modeling/gpt_bigcode.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/modeling\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/modeling/moss.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/modeling\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/modeling/gpt2.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/modeling\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/modeling/llama.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/modeling\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/modeling/gptj.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/modeling\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/modeling/__init__.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/modeling\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/modeling/rw.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/modeling\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/auto_gptq/nn_modules/triton_utils\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/nn_modules/triton_utils/kernels.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/nn_modules/triton_utils\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/nn_modules/triton_utils/mixin.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/nn_modules/triton_utils\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/nn_modules/triton_utils/__init__.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/nn_modules/triton_utils\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/nn_modules/triton_utils/custom_autotune.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/nn_modules/triton_utils\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/auto_gptq/eval_tasks/_utils\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/eval_tasks/_utils/classification_utils.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/eval_tasks/_utils\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/eval_tasks/_utils/generation_utils.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/eval_tasks/_utils\n",
      "  \u001b[31m   \u001b[0m copying auto_gptq/eval_tasks/_utils/__init__.py -> build/lib.linux-x86_64-cpython-310/auto_gptq/eval_tasks/_utils\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-qq3mnez9/auto-gptq_5397290c1b2640fab6550f417851c50c/setup.py\", line 92, in <module>\n",
      "  \u001b[31m   \u001b[0m     setup(\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/setuptools/__init__.py\", line 103, in setup\n",
      "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 185, in setup\n",
      "  \u001b[31m   \u001b[0m     return run_commands(dist)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 201, in run_commands\n",
      "  \u001b[31m   \u001b[0m     dist.run_commands()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 969, in run_commands\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/setuptools/dist.py\", line 989, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/wheel/bdist_wheel.py\", line 364, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(\"build\")\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/setuptools/dist.py\", line 989, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/setuptools/_distutils/command/build.py\", line 131, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd_name)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/setuptools/dist.py\", line 989, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/setuptools/command/build_ext.py\", line 88, in run\n",
      "  \u001b[31m   \u001b[0m     _build_ext.run(self)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/setuptools/_distutils/command/build_ext.py\", line 345, in run\n",
      "  \u001b[31m   \u001b[0m     self.build_extensions()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 523, in build_extensions\n",
      "  \u001b[31m   \u001b[0m     _check_cuda_version(compiler_name, compiler_version)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 414, in _check_cuda_version\n",
      "  \u001b[31m   \u001b[0m     raise RuntimeError(CUDA_MISMATCH_MESSAGE.format(cuda_str_version, torch.version.cuda))\n",
      "  \u001b[31m   \u001b[0m RuntimeError:\n",
      "  \u001b[31m   \u001b[0m The detected CUDA version (11.5) mismatches the version that was used to compile\n",
      "  \u001b[31m   \u001b[0m PyTorch (12.1). Please make sure to use the same CUDA versions.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for auto-gptq\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for auto-gptq\n",
      "Failed to build auto-gptq\n",
      "\u001b[31mERROR: Could not build wheels for auto-gptq, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install auto-gptq==0.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:732: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = \"meta-llama/Llama-2-7b-chat-hf\" # meta-llama/Llama-2-7b-chat-hf\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2431c453d32e4c1590995343e5ee8fee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch \n",
    "\n",
    "llama_pipeline = pipeline(\n",
    "    \"text-generation\",  # LLM task\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    temperature=0.1,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "here is the chat history\n",
    "{chat_history}\n",
    "\n",
    "{prompt} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"prompt\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=HuggingFacePipeline(pipeline=llama_pipeline)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",    k=3,\n",
    "    return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm=llm, prompt=prompt, verbose=False,  memory=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while 1:\n",
    "#   text=input(\"You: \")\n",
    "#   if text=='end':\n",
    "#     break\n",
    "#   output=llm_chain.predict(prompt=text)\n",
    "#   print(\"Chatbot: \",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=llm_chain.predict(prompt=\"Hello\")\n",
    "print(output.split(\"[/INST]\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello! I'm here to help you with any questions or concerns you may have. Please feel free to ask me anything, and I will do my best to provide helpful and respectful responses. I'm programmed to be socially unbiased and positive in nature, and I will always strive to ensure that my answers are safe and free from harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. If a question does not make sense or is not factually coherent, I will explain why instead of answering something not correct. If I don't know the answer to a question, I will not share false information. Is there anything specific you would like to know or discuss?\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs, history):\n",
    "\n",
    "  response = llm_chain.predict(prompt=inputs)\n",
    "  print(response.split(\"[/INST]\")[-1])\n",
    "  return response.split(\"[/INST]\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://310daa06be60c819fa.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://310daa06be60c819fa.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello! It's nice to chat with you. Is there something specific you would like to talk about or ask? I'm here to help with any questions or concerns you may have. Please feel free to ask me anything, and I will do my best to provide helpful and respectful responses.\n",
      "\n",
      "Hello! I'm here to help you with any questions or concerns you may have. Is there something specific you need help with? Please feel free to ask me anything, and I will do my best to provide helpful and respectful responses. Is there something specific you would like to know or discuss?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/gradio/queueing.py\", line 522, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/gradio/blocks.py\", line 1741, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/gradio/blocks.py\", line 1294, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/gradio/utils.py\", line 718, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/gradio/chat_interface.py\", line 507, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/tmp/ipykernel_1044408/712154486.py\", line 3, in predict\n",
      "    response = llm_chain.predict(prompt=inputs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/langchain/chains/llm.py\", line 293, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/langchain/chains/base.py\", line 378, in __call__\n",
      "    return self.invoke(\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/langchain/chains/base.py\", line 163, in invoke\n",
      "    raise e\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/langchain/chains/base.py\", line 153, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/langchain/chains/llm.py\", line 103, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/langchain/chains/llm.py\", line 115, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 569, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 748, in generate\n",
      "    output = self._generate_helper(\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 606, in _generate_helper\n",
      "    raise e\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 593, in _generate_helper\n",
      "    self._generate(\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/langchain_community/llms/huggingface_pipeline.py\", line 267, in _generate\n",
      "    responses = self.pipeline(\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 240, in __call__\n",
      "    return super().__call__(text_inputs, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1187, in __call__\n",
      "    outputs = list(final_iterator)\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 124, in __next__\n",
      "    item = next(self.iterator)\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 125, in __next__\n",
      "    processed = self.infer(item, **self.params)\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1112, in forward\n",
      "    model_outputs = self._forward(model_inputs, **forward_params)\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 327, in _forward\n",
      "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1575, in generate\n",
      "    result = self._sample(\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2697, in _sample\n",
      "    outputs = self(\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 1215, in forward\n",
      "    logits = self.lm_head(hidden_states)\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 116, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "RuntimeError: NVML_SUCCESS == DriverAPI::get()->nvmlInit_v2_() INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1708025847130/work/c10/cuda/CUDACachingAllocator.cpp\":799, please report a bug to PyTorch. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://310daa06be60c819fa.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "demo = gr.ChatInterface(fn=predict, title=\"Echo Bot\")\n",
    "demo.launch(share=True,debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
